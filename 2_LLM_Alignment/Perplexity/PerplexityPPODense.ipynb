{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c055b77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Program Files\\Python313\\Lib\\site-packages\\torch\\cuda\\__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f76eb85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SFT\n",
    "# model_path = None \n",
    "# model_name = \"SFT-Baseline\"\n",
    "\n",
    "# DPO\n",
    "# model_path = \"../../models/smollm2-dpo-final\"  # Path to your saved DPO adapter\n",
    "# model_name = \"DPO\"\n",
    "\n",
    "# PPO Sparse\n",
    "# model_path = \"../../models/smollm2-ppo-sparse-final\"\n",
    "# model_name = \"PPO-Sparse\"\n",
    "\n",
    "# PPO Dense\n",
    "model_path = \"../../models/smollm2-ppo-dense-final\"\n",
    "model_name = \"PPO-Dense\"\n",
    "\n",
    "# GRPO\n",
    "# model_path = \"../../models/smollm2-grpo-final\"\n",
    "# model_name = \"GRPO\"\n",
    "\n",
    "base_model_id = \"HuggingFaceTB/smollm2-135M-SFT-Only\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d96baa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Reference Model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading PPO-Dense Policy...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): LlamaForCausalLM(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): Embedding(49152, 576, padding_idx=2)\n",
       "        (layers): ModuleList(\n",
       "          (0-29): 30 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=576, out_features=576, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=576, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=576, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): Linear(in_features=576, out_features=192, bias=False)\n",
       "              (v_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=576, out_features=192, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=576, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=192, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): Linear(in_features=576, out_features=576, bias=False)\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
       "              (up_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
       "              (down_proj): Linear(in_features=1536, out_features=576, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((576,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((576,), eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm((576,), eps=1e-05)\n",
       "        (rotary_emb): LlamaRotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=576, out_features=49152, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# --- 2. Load Models ---\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load Reference Model (Frozen SFT Base) - Required for KL & Perplexity\n",
    "print(\"Loading Reference Model...\")\n",
    "ref_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_id, torch_dtype=torch.float16, device_map=device\n",
    ")\n",
    "ref_model.eval()\n",
    "\n",
    "# Load Aligned Policy (Your Trained Adapter)\n",
    "print(f\"Loading {model_name} Policy...\")\n",
    "policy_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_id, torch_dtype=torch.float16, device_map=device\n",
    ")\n",
    "try:\n",
    "    policy_model = PeftModel.from_pretrained(policy_model, model_path)\n",
    "except:\n",
    "    print(\"Warning: Loading base model without adapter (SFT Baseline mode)\")\n",
    "policy_model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f45bf5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- 3. Prepare Test Set ---\n",
    "# Using held-out subset as requested\n",
    "dataset = load_dataset(\"Intel/orca_dpo_pairs\", split=\"train[2000:2050]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "628f793a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [03:49<00:00,  4.60s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "results = []\n",
    "print(\"Starting Evaluation...\")\n",
    "\n",
    "for i, example in tqdm(enumerate(dataset), total=len(dataset)):\n",
    "    # Format Prompt\n",
    "    messages = [{\"role\": \"user\", \"content\": example[\"question\"]}]\n",
    "    prompt_text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    inputs = tokenizer(prompt_text, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    # A. GENERATION (For Verbosity Analysis)\n",
    "    with torch.no_grad():\n",
    "        outputs = policy_model.generate(\n",
    "            **inputs, \n",
    "            max_new_tokens=128, \n",
    "            do_sample=True, \n",
    "            temperature=0.7\n",
    "        )\n",
    "    \n",
    "    response_tokens = outputs[0][inputs.input_ids.shape[1]:]\n",
    "    response_text = tokenizer.decode(response_tokens, skip_special_tokens=True)\n",
    "    token_count = len(response_tokens)\n",
    "\n",
    "    # B. METRICS CALCULATION (Perplexity & KL)\n",
    "    # We use the 'chosen' response from the dataset as the \"Instruction Following Data\" \n",
    "    # to measure Perplexity\n",
    "    target_text = example[\"chosen\"]\n",
    "    full_seq = prompt_text + target_text\n",
    "    \n",
    "    # Ensure no padding is added for single-sample inference\n",
    "    encodings = tokenizer(full_seq, return_tensors=\"pt\", padding=False).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Get raw logits\n",
    "        policy_outputs = policy_model(**encodings)\n",
    "        ref_outputs = ref_model(**encodings)\n",
    "        \n",
    "        # 1. PERPLEXITY\n",
    "        # Shift so we predict the next token\n",
    "        shift_logits = policy_outputs.logits[..., :-1, :].contiguous()\n",
    "        shift_labels = encodings.input_ids[..., 1:].contiguous()\n",
    "        \n",
    "        # Mask the prompt part so we only calculate perplexity on the response\n",
    "        prompt_len = inputs.input_ids.shape[1]\n",
    "        # We need to subtract 1 because we shifted the labels\n",
    "        response_start_idx = prompt_len - 1 \n",
    "        \n",
    "        # Calculate loss only on the response tokens\n",
    "        loss_fct = torch.nn.CrossEntropyLoss()\n",
    "        \n",
    "        # Slice the logits and labels to strictly the response part\n",
    "        resp_logits = shift_logits[:, response_start_idx:, :]\n",
    "        resp_labels = shift_labels[:, response_start_idx:]\n",
    "        \n",
    "        loss = loss_fct(resp_logits.view(-1, resp_logits.size(-1)), resp_labels.view(-1))\n",
    "        perplexity = torch.exp(loss).item()\n",
    "        \n",
    "        # 2. KL DIVERGENCE (Stable Calculation)\n",
    "        # CRITICAL FIX: Cast to Float32 to prevent NaN in Softmax\n",
    "        p_logits = policy_outputs.logits.float()\n",
    "        q_logits = ref_outputs.logits.float()\n",
    "        \n",
    "        # Slice to response only (we don't care about KL on the prompt)\n",
    "        # We look at the distribution at every step of the response\n",
    "        p_resp = p_logits[:, response_start_idx:, :]\n",
    "        q_resp = q_logits[:, response_start_idx:, :]\n",
    "        \n",
    "        # Calculate probabilities and log-probabilities\n",
    "        # KL(P || Q) = sum( P(x) * (log P(x) - log Q(x)) )\n",
    "        \n",
    "        p_probs = F.softmax(p_resp, dim=-1)\n",
    "        p_log_probs = F.log_softmax(p_resp, dim=-1)\n",
    "        q_log_probs = F.log_softmax(q_resp, dim=-1)\n",
    "        \n",
    "        # Calculate KL per token, then mean over the sequence\n",
    "        # We use the reduction \"none\" to handle sequence length manually if needed, \n",
    "        # but mean() usually works fine here.\n",
    "        kl_per_token = torch.sum(p_probs * (p_log_probs - q_log_probs), dim=-1)\n",
    "        kl_div = kl_per_token.mean().item()\n",
    "\n",
    "    results.append({\n",
    "        \"Model\": model_name,\n",
    "        \"Prompt\": example[\"question\"],\n",
    "        \"Response\": response_text,\n",
    "        \"Token_Count\": token_count,\n",
    "        \"Perplexity\": perplexity,\n",
    "        \"KL_Divergence\": kl_div\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88bb4026",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Complete.\n",
      "Mean Perplexity: 5.18\n",
      "Mean KL Divergence: 0.0172\n",
      "Mean Length: 66.7\n"
     ]
    }
   ],
   "source": [
    "# Save\n",
    "df = pd.DataFrame(results)\n",
    "df.to_csv(f\"perplexity_results_{model_name}.csv\", index=False)\n",
    "print(\"Evaluation Complete.\")\n",
    "print(f\"Mean Perplexity: {df['Perplexity'].mean():.2f}\")\n",
    "print(f\"Mean KL Divergence: {df['KL_Divergence'].mean():.4f}\")\n",
    "print(f\"Mean Length: {df['Token_Count'].mean():.1f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
