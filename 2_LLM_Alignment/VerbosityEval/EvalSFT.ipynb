{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "efc9887e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Program Files\\Python313\\Lib\\site-packages\\torch\\cuda\\__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForSequenceClassification\n",
    "from peft import PeftModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0fe4f97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# SFT\n",
    "model_path = None \n",
    "model_type = \"SFT-Baseline\"\n",
    "\n",
    "# DPO\n",
    "# model_path = \"../../models/smollm2-dpo-final\"\n",
    "# model_type = \"DPO\"\n",
    "\n",
    "# PPO Sparse\n",
    "# model_path = \"../../models/smollm2-ppo-sparse-final\"\n",
    "# model_type = \"PPO-Sparse\"\n",
    "\n",
    "# PPO Dense\n",
    "# model_path = \"../../models/smollm2-ppo-dense-final\"\n",
    "# model_type = \"PPO-Dense\"\n",
    "\n",
    "# GRPO\n",
    "# model_path = \"../../models/smollm2-grpo-final\"\n",
    "# model_type = \"GRPO\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5ed8fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = f\"eval_results_{model_type}.csv\"\n",
    "reward_model_path = \"../../models/smollm2-reward-model-final\"\n",
    "base_model_id = \"HuggingFaceTB/smollm2-135M-SFT-Only\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c25382b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading SFT-Baseline Policy...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Reward Model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at HuggingFaceTB/smollm2-135M-SFT-Only and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForSequenceClassification(\n",
       "  (base_model): LoraModel(\n",
       "    (model): LlamaForSequenceClassification(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): Embedding(49152, 576, padding_idx=2)\n",
       "        (layers): ModuleList(\n",
       "          (0-29): 30 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=576, out_features=576, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=576, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=576, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=576, out_features=192, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=576, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=192, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=576, out_features=192, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=576, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=192, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=576, out_features=576, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=576, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=576, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=576, out_features=1536, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=576, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1536, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=576, out_features=1536, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=576, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1536, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1536, out_features=576, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1536, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=576, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((576,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((576,), eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm((576,), eps=1e-05)\n",
       "        (rotary_emb): LlamaRotaryEmbedding()\n",
       "      )\n",
       "      (score): ModulesToSaveWrapper(\n",
       "        (original_module): Linear(in_features=576, out_features=1, bias=False)\n",
       "        (modules_to_save): ModuleDict(\n",
       "          (default): Linear(in_features=576, out_features=1, bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "eval_dataset = load_dataset(\"Intel/orca_dpo_pairs\", split=\"train[2050:2100]\")\n",
    "\n",
    "print(f\"Loading {model_type} Policy...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "policy_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_id, \n",
    "    torch_dtype=torch.float16, \n",
    "    device_map=device\n",
    ")\n",
    "\n",
    "if model_path:\n",
    "    policy_model = PeftModel.from_pretrained(policy_model, model_path)\n",
    "    \n",
    "policy_model.eval()\n",
    "\n",
    "print(\"Loading Reward Model...\")\n",
    "rm_base = AutoModelForSequenceClassification.from_pretrained(\n",
    "    base_model_id, \n",
    "    num_labels=1, \n",
    "    torch_dtype=torch.float16, \n",
    "    device_map=device\n",
    ")\n",
    "reward_model = PeftModel.from_pretrained(rm_base, reward_model_path)\n",
    "reward_model.eval()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e967eba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating responses for 50 prompts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [04:46<00:00,  5.72s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "results = []\n",
    "\n",
    "print(f\"Generating responses for {len(eval_dataset)} prompts...\")\n",
    "\n",
    "for example in tqdm(eval_dataset):\n",
    "    prompt_text = f\"<|im_start|>system\\n{example['system']}<|im_end|>\\n<|im_start|>user\\n{example['question']}<|im_end|>\\n<|im_start|>assistant\\n\"\n",
    "    \n",
    "    inputs = tokenizer(prompt_text, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    # 1. Generate Response\n",
    "    with torch.no_grad():\n",
    "        outputs = policy_model.generate(\n",
    "            **inputs, \n",
    "            max_new_tokens=256,   # Keep consistent for comparison\n",
    "            do_sample=True,      # Sampling allows diversity\n",
    "            temperature=0.7,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    # Decode response (removing the prompt)\n",
    "    full_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    generated_response = tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)\n",
    "    \n",
    "    # 2. Get Reward Score\n",
    "    # The Reward Model expects the FULL text (Prompt + Response)\n",
    "    # We re-encode to be safe\n",
    "    rm_inputs = tokenizer(full_text, return_tensors=\"pt\", truncation=True, max_length=1024).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        rm_output = reward_model(**rm_inputs)\n",
    "        score = rm_output.logits[0].item()\n",
    "    \n",
    "    # 3. Store Data\n",
    "    results.append({\n",
    "        \"Model\": model_type,\n",
    "        \"Prompt\": example[\"question\"],\n",
    "        \"Response\": generated_response,\n",
    "        \"Reward_Score\": score,\n",
    "        \"Length (Tokens)\": len(outputs[0]) - inputs.input_ids.shape[1] # Actual tokens generated\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "daff80c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved results to eval_results_SFT-Baseline.csv\n",
      "Average Reward: 1.7408\n",
      "Average Length: 112.6\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- 4. Save Results ---\n",
    "df = pd.DataFrame(results)\n",
    "df.to_csv(output_file, index=False)\n",
    "print(f\"Saved results to {output_file}\")\n",
    "print(f\"Average Reward: {df['Reward_Score'].mean():.4f}\")\n",
    "print(f\"Average Length: {df['Length (Tokens)'].mean():.1f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
